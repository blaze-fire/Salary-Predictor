# -*- coding: utf-8 -*-
"""Job_ML_Project.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1i-STIP0QMVjMeX6DJ5rlhWds0WKqPhOA
"""

import pandas as pd
import numpy as np
import re
from nltk.corpus import stopwords
from nltk.stem import WordNetLemmatizer
import nltk
nltk.download('stopwords')
nltk.download('punkt')
nltk.download('wordnet')
lemmatizer = WordNetLemmatizer()

raw_df = pd.read_csv('data/drive_data/raw_data(1).csv')

raw_df.head()

class Preprocess:

    def preprocess_text(self, text):
        text = text.lower()
        text = re.sub('[^a-zA-Z]', ' ', text)
        text = ' '.join([word for word in text.split() if not word in set(stopwords.words('english'))])
        text = ' '.join([lemmatizer.lemmatize(w) for w in text.split()])

        return text
    
    def clean_data(self, df):
        #Some company posted for the same profile many times after a gap of few days   
        # we can store this information in the column 'posting frequency'
        # calculate posting frequency on the basis of company
        freq = df['Company'].value_counts()

        df.drop_duplicates(inplace=True)
        df.reset_index(drop=True, inplace=True)

        df['posting_frequency'] = df['Company'].map(freq)

        # those not repeated will be null, therefore fill them as 1
        df['posting_frequency'].fillna(1, inplace=True)

        # We just deleted duplicates but we still see multiple entries for some companies
        # It looks like recently posted jobs with new tag are causing this, 
        # lets remove them
        df['Job_position'] = df['Job_position'].apply(lambda x: str(x).replace('\nnew','').lower())


        df['rating'] = df['rating'].astype('float')


        """
         Rows with missing salaries contain valuable information regarding job position, location and their requirements
         So we will keep them for now 
         for now lets fill them with -999
        """

        df['Salary'].fillna('-999', inplace=True)

        # remove new line and ruppes symbol  
        df['Salary'] = df['Salary'].apply(lambda x: str(x).replace('\n','').replace('₹',''))

        return df



    # to calculate max and min Salary per annum
    def calc_salary(self, df):

        yearly_min = {}
        yearly_max = {}

        for i in range(0, len(df)):

            if df['Salary'][i] == '-999':
                yearly_min[i] = 0
                yearly_max[i] = 0

            if 'a year' in df['Salary'][i]:
                sal_min = df['Salary'][i].split('-')[0].replace('a year','').replace(',','')
                yearly_min[i] = int(sal_min)

                try:
                    sal_max = df['Salary'][i].split('-')[1].replace('a year','').replace(',','')
                    yearly_max[i] = int(sal_max)

                # if only single value present will be stored in both max and min, so the average comes accuate
                except:
                    sal_max = df['Salary'][i].split('-')[0].replace('a year','').replace(',','')
                    yearly_max[i] = int(sal_max)


            if 'a month' in df['Salary'][i]:
                sal_min = df['Salary'][i].split('-')[0].replace('a month','').replace(',','')
                yearly_min[i] = int(sal_min) * 12

                try:
                    sal_max = df['Salary'][i].split('-')[1].replace('a month','').replace(',','')
                    yearly_max[i] = int(sal_max) * 12    

                # if only single value present will be stored in both max and min, so the average comes accuate
                except:
                    sal_max = df['Salary'][i].split('-')[0].replace('a month','').replace(',','')
                    yearly_max[i] = int(sal_max) * 12


            if 'an hour' in df['Salary'][i]:
                sal_min = df['Salary'][i].split('-')[0].replace('an hour','').replace(',','').replace(' ','')
                yearly_min[i] = float(sal_min) * 9 * 22 * 12

                try:
                    sal_max = df['Salary'][i].split('-')[1].replace('an hour','').replace(',','').replace(' ','')
                    yearly_max[i] = float(sal_max) * 9 * 22 * 12  

                # if only single value present will be stored in both max and min, so the average comes accuate
                except:
                    sal_max = df['Salary'][i].split('-')[0].replace('an hour','').replace(',','').replace(' ','')
                    yearly_max[i] = float(sal_max) * 9 * 22 * 12

        # min, max and avg Salary columns
        df['min_Salary'] = pd.DataFrame(yearly_min.values(), index= yearly_min.keys())
        df['max_Salary'] = pd.DataFrame(yearly_max.values(), index= yearly_max.keys())
        df['avg_yearly_sal'] = ( df['min_Salary'] + df['max_Salary'] )/2.

        #Lets just drop these as we got our target column
        df.drop(['max_Salary', 'min_Salary', 'Salary'], axis=1, inplace=True)

        df['avg_yearly_sal'].fillna(0, inplace=True)

        return df



    def work_location(self, df):

        work_type = []
        for j in range(len(df)):

            if re.findall('full.time?', df['requirements'][j].lower()):
                work_type.append(2)

            elif re.findall('remote?', df['requirements'][j].lower()):
                work_type.append(1)

            else:
                work_type.append(0)


        df['work_category'] = work_type

        #df = pd.concat([df, pd.get_dummies(df['work_category'])], axis=1) 
        #df.drop('work_category', axis=1, inplace=True)

        return df



    def calc_experience(self, df):
        df['experience'].fillna('', inplace = True)

        experience_list = []
        for j in range(len(df)):

            if re.findall(r'\d+ year', df['experience'][j]):
                experience_list.append(re.search(r'\d+ year', df['experience'][j]).group()[0])

            else:
                experience_list.append('')

        df['net_experience'] = experience_list
        df['net_experience'] = pd.to_numeric(df['net_experience'])
                
        return df



    #Educational criteria mentioned by these companies can also be useful
    def education(self, df):

        education_list = []
        education_dict = {'bachelor':1, 'master':2, 'graduate':3}
        for j in range(len(df)):
            
            if re.findall(r'(graduate|bachelor|master)', df['experience'][j].lower()):
                education_list.append( education_dict[re.search(r'(graduate|bachelor|master)', df['experience'][j].lower()).group()] )
            
            elif re.findall(r'(graduate|bachelor|master)', df['requirements'][j].lower()):
                education_list.append( education_dict[re.search(r'(graduate|bachelor|master)', df['requirements'][j].lower()).group()] )
            
            else:
                education_list.append(0)


        df['education_level'] = education_list

        #df = pd.concat([df, pd.get_dummies(df['education_level'])], axis=1) 
        #df.drop('education_level', axis=1, inplace=True)

        return df



    def seniority(self, df):

        seniority_list=[]
        for j in range(len(df)):

            if re.findall(r'senior', df['requirements'][j].lower()):
                seniority_list.append(2)
            
            elif re.findall(r'junior', df['requirements'][j].lower()):
                seniority_list.append(1)

            else:
                seniority_list.append(0)

        df['job_title'] = seniority_list

        #df = pd.concat([df, pd.get_dummies(df['job_title'])], axis=1) 
        #df.drop('job_title', axis=1, inplace=True)

        return df


    def get_states(self, df):

        with open('utils/states.txt', 'r') as f:
            states = f.read()
            states_list = states.split(',')
        f.close()
        
        job_states = []

        for j in range(len(df)):
            counter = 0
            
            for i in states_list:
                if re.findall(i, df['Location'][j].lower()):
                    job_states.append(states_list.index(i))
                    counter = 1
                    break
            
            if counter == 0:
                job_states.append(states_list.index('State_missing'))

        df['State'] = job_states
        
        return df




    def city(self, df):

        with open('utils/cities.txt', 'r') as f:
            cities = f.read()
            cities_list = cities.split(',')
        f.close()

        job_cities = []

        for j in range(len(df)):
            counter = 0
            
            for i in cities_list:
                if re.findall(i, df['Location'][j].lower()):
                    job_cities.append(cities_list.index(i))
                    counter = 1
                    break
            
            if counter == 0:
                job_cities.append(cities_list.index('city_missing'))
                
        df['Cities'] = job_cities
        
        return df
    
    


    def analyze_skills(self, df):
        key_df = pd.read_csv('utils/keywords.csv')
        
        key_df.fillna('na', inplace=True)
        
        for col in key_df.columns:
            df[col] = np.zeros(len(df), dtype='int')
            
        for j in range(len(df)):
            for col in key_df.columns:
                for skill in key_df[col]:
                    try:
                        if skill != 'na':
                            if re.findall(skill, df['requirements'][j].lower()):
                                df.loc[j, col] += 1
                            
                            if re.findall(skill, df['Job_position'][j].lower()):
                                df.loc[j, col] += 1
                    except:
                        pass
        return df
    

    def final_operations(self, df):
    
        # remove columns with constant values
        df['Company'] = df['Company'].apply(lambda x: self.preprocess_text(str(x)))
        df['Job_position'] = df['Job_position'].apply(lambda x: self.preprocess_text(str(x)))
        df['requirements'] = df['requirements'].fillna('')
        df['requirements'] = df['requirements'].apply(lambda x: self.preprocess_text(str(x)))
        df['job_descr_len'] = df['requirements'].apply(lambda x: 0 if not x else len(x))
        df.drop(['experience', 'Location'], axis=1, inplace=True)

        df = df.loc[:, (df != df.iloc[0]).any()] 

        return df
    
    def __call__(self, df):
        df = self.clean_data(df)
        df = self.calc_salary(df)
        df = self.work_location(df)
        df = self.calc_experience(df)
        df = self.education(df)
        df = self.seniority(df)
        df = self.get_states(df)
        df = self.city(df)
        df = self.analyze_skills(df)
        df = self.final_operations(df)
        return df

raw_df = Preprocess()(raw_df)

raw_df.head()

raw_df.columns





class Preprocess_final:

    def preprocess_text(self, text):
        text = text.lower()
        text = re.sub('[^a-zA-Z]', ' ', text)
        text = ' '.join([word for word in text.split() if not word in set(stopwords.words('english'))])
        text = ' '.join([lemmatizer.lemmatize(w) for w in text.split()])

        return text
    
    def clean_data(self, df):
        #Some company posted for the same profile many times after a gap of few days   
        # we can store this information in the column 'posting frequency'
        # calculate posting frequency on the basis of company
        freq = df['Company'].value_counts()

        df.drop_duplicates(inplace=True)
        df.reset_index(drop=True, inplace=True)

        df['posting_frequency'] = df['Company'].map(freq)

        # those not repeated will be null, therefore fill them as 1
        df['posting_frequency'].fillna(1, inplace=True)

        # We just deleted duplicates but we still see multiple entries for some companies
        # It looks like recently posted jobs with new tag are causing this, 
        # lets remove them
        df['Job_position'] = df['Job_position'].apply(lambda x: str(x).replace('\nnew','').lower())


        df['rating'] = df.rating.apply(lambda x: str(x).replace('\n',''))
        df['rating'] = pd.to_numeric(df['rating'].replace({"na":np.nan}))


        """
         Rows with missing salaries contain valuable information regarding job position, location and their requirements
         So we will keep them for now 
         for now lets fill them with -999
        """

        df['Salary'].fillna('-999', inplace=True)

        # remove new line and ruppes symbol  
        df['Salary'] = df['Salary'].apply(lambda x: str(x).replace('\n','').replace('₹',''))

        return df



    # to calculate max and min Salary per annum
    def calc_salary(self, df):

        yearly_min = {}
        yearly_max = {}

        for i in range(0, len(df)):

            if df['Salary'][i] == '-999':
                yearly_min[i] = 0
                yearly_max[i] = 0

            if 'a year' in df['Salary'][i]:
                sal_min = df['Salary'][i].split('-')[0].replace('a year','').replace(',','')
                yearly_min[i] = int(sal_min)

                try:
                    sal_max = df['Salary'][i].split('-')[1].replace('a year','').replace(',','')
                    yearly_max[i] = int(sal_max)

                # if only single value present will be stored in both max and min, so the average comes accuate
                except:
                    sal_max = df['Salary'][i].split('-')[0].replace('a year','').replace(',','')
                    yearly_max[i] = int(sal_max)


            if 'a month' in df['Salary'][i]:
                sal_min = df['Salary'][i].split('-')[0].replace('a month','').replace(',','')
                yearly_min[i] = int(sal_min) * 12

                try:
                    sal_max = df['Salary'][i].split('-')[1].replace('a month','').replace(',','')
                    yearly_max[i] = int(sal_max) * 12    

                # if only single value present will be stored in both max and min, so the average comes accuate
                except:
                    sal_max = df['Salary'][i].split('-')[0].replace('a month','').replace(',','')
                    yearly_max[i] = int(sal_max) * 12


            if 'an hour' in df['Salary'][i]:
                sal_min = df['Salary'][i].split('-')[0].replace('an hour','').replace(',','').replace(' ','')
                yearly_min[i] = float(sal_min) * 9 * 22 * 12

                try:
                    sal_max = df['Salary'][i].split('-')[1].replace('an hour','').replace(',','').replace(' ','')
                    yearly_max[i] = float(sal_max) * 9 * 22 * 12  

                # if only single value present will be stored in both max and min, so the average comes accuate
                except:
                    sal_max = df['Salary'][i].split('-')[0].replace('an hour','').replace(',','').replace(' ','')
                    yearly_max[i] = float(sal_max) * 9 * 22 * 12

        # min, max and avg Salary columns
        df['min_Salary'] = pd.DataFrame(yearly_min.values(), index= yearly_min.keys())
        df['max_Salary'] = pd.DataFrame(yearly_max.values(), index= yearly_max.keys())
        df['avg_yearly_sal'] = ( df['min_Salary'] + df['max_Salary'] )/2.

        #Lets just drop these as we got our target column
        df.drop(['max_Salary', 'min_Salary', 'Salary'], axis=1, inplace=True)

        df['avg_yearly_sal'].fillna(0, inplace=True)

        return df



    def work_location(self, df):

        work_type = []
        for j in range(len(df)):

            if re.findall('full.time?', df['requirements'][j].lower()):
                work_type.append(2)

            elif re.findall('remote?', df['requirements'][j].lower()):
                work_type.append(1)

            else:
                work_type.append(0)


        df['work_category'] = work_type

        #df = pd.concat([df, pd.get_dummies(df['work_category'])], axis=1) 

        #df.drop('work_category', axis=1, inplace=True)

        return df



    def calc_experience(self, df):

        experience_list = []
        for j in range(len(df)):

            if re.findall(r'\d+ year', df['requirements'][j]):
                experience_list.append(re.search(r'\d+ year', df['requirements'][j]).group()[0])

            else:
                experience_list.append('')

        df['net_experience'] = experience_list
        df['net_experience'] = pd.to_numeric(df['net_experience'])

        return df



    #Educational criteria mentioned by these companies can also be useful
    def education(self, df):

        education_list = []
        education_dict = {'bachelor':1, 'master':2, 'graduate':3}
        for j in range(len(df)):
            
            if re.findall(r'(graduate|bachelor|master)', df['requirements'][j].lower()):
                education_list.append( education_dict[re.search(r'(graduate|bachelor|master)', df['requirements'][j].lower()).group()] )
            
            else:
                education_list.append(0)


        df['education_level'] = education_list

        #df = pd.concat([df, pd.get_dummies(df['education_level'])], axis=1) 
        #df.drop('education_level', axis=1, inplace=True)

        return df



    def seniority(self, df):

        seniority_list=[]
        for j in range(len(df)):

            if re.findall(r'senior', df['requirements'][j].lower()):
                seniority_list.append(2)
            
            elif re.findall(r'junior', df['requirements'][j].lower()):
                seniority_list.append(1)

            else:
                seniority_list.append(0)

        df['job_title'] = seniority_list

        #df = pd.concat([df, pd.get_dummies(df['job_title'])], axis=1) 
        #df.drop('job_title', axis=1, inplace=True)

        return df


    def get_states(self, df):

        with open('utils/states.txt', 'r') as f:
            states = f.read()
            states_list = states.split(',')
        f.close()

        job_states = []

        for j in range(len(df)):
            counter = 0

            for i in states_list:
                if re.findall(i, df['Company'][j].lower()):
                    job_states.append(states_list.index(i))
                    df.loc[j, 'Company'] = re.sub(i, '',  df['Company'][j].lower())
                    counter = 1
                    break
                
            if counter == 0:
                job_states.append(states_list.index('State_missing'))

        df['State'] = job_states

        return df


    def replace_comp(self, df):

        for j in range(len(df)):

            if re.findall(r'(ltd|review|reviews|remote|temporarily|\d.?)', df['Company'][j].lower()):
                df.loc[j, 'Company'] = re.sub(r'(ltd|review|reviews|remote|temporarily|\d.?)', '', df['Company'][j].lower())
            
        return df


    def city(self, df):

        with open('utils/cities.txt', 'r') as f:
            cities = f.read()
            cities_list = cities.split(',')
        f.close()

        job_cities = []

        for j in range(len(df)):
            counter = 0

            for i in cities_list:
                if re.findall(i, df['Company'][j].lower()):
                    job_cities.append(cities_list.index(i))
                    df.loc[j, 'Company'] = re.sub(i, ' ', df['Company'][j].lower())
                    counter = 1
                    break
                
            if counter == 0:
                job_cities.append(cities_list.index('city_missing'))

        df['Cities'] = job_cities

        return df

    
    def analyze_skills(self, df):
        key_df = pd.read_csv('utils/keywords.csv')
        
        key_df.fillna('na', inplace=True)
        
        for col in key_df.columns:
            df[col] = np.zeros(len(df), dtype='int')
            
        for j in range(len(df)):
            for col in key_df.columns:
                for skill in key_df[col]:
                    try:
                        if skill != 'na':
                            if re.findall(skill, df['requirements'][j].lower()):
                                df.loc[j, col] += 1
                            
                            if re.findall(skill, df['Job_position'][j].lower()):
                                df.loc[j, col] += 1
                    except:
                        pass
        return df
    

    def final_operations(self, df):
    
        # remove columns with constant values
        df['Company'] = df['Company'].apply(lambda x: self.preprocess_text(str(x)))
        df['Job_position'] = df['Job_position'].apply(lambda x: self.preprocess_text(str(x)))
        df['requirements'] = df['requirements'].fillna('')
        df['requirements'] = df['requirements'].apply(lambda x: self.preprocess_text(str(x)))
        df['job_descr_len'] = df['requirements'].apply(lambda x: 0 if not x else len(x))

        df = df.loc[:, (df != df.iloc[0]).any()] 

        return df
    
    def __call__(self, df):
        df = self.clean_data(df)
        df = self.calc_salary(df)
        df = self.work_location(df)
        df = self.calc_experience(df)
        df = self.education(df)
        df = self.seniority(df)
        df = self.get_states(df)
        df = self.replace_comp(df)
        df = self.city(df)
        df = self.analyze_skills(df)
        df = self.final_operations(df)
        return df



another_df = pd.read_csv('data/drive_data/final.csv')

len(another_df)

another_df = Preprocess_final()(another_df)

another_df.columns

raw_df.columns

final_df = pd.concat([raw_df, another_df], axis=0)

len(final_df)

from sklearn.utils import shuffle

df = shuffle(final_df)

df.reset_index(drop=True, inplace=True)

df.head()

df.select_dtypes(include='object').columns

df.isnull().sum().sort_values()

len(df[df['avg_yearly_sal']>0])



"""#Calculating missing ratings"""

temp_df = df[df['avg_yearly_sal']>0]
temp_df.reset_index(drop=True, inplace=True)
train = temp_df[~temp_df['rating'].isnull()]
test = temp_df[temp_df['rating'].isnull()]
train_set = train.drop(['rating', 'avg_yearly_sal'], axis=1)
y_train = train['rating']
test_set = test.drop(['rating', 'avg_yearly_sal'], axis=1)

from utils.nlp_utils import Word2VecVectorizer
"""#Glove Embeddings"""

from gensim.models import KeyedVectors
# load GloVe model
filename = 'word2vec_50d.bin'
model = KeyedVectors.load_word2vec_format(filename, binary=True)


rating_train = train_set.select_dtypes(exclude='object').values
rating_test = test_set.select_dtypes(exclude='object').values

def glove_embedded(X_train, col, X_test, rating_train, rating_test):

  vectorizer = Word2VecVectorizer(model)

  X_train_embed = vectorizer.fit_transform(X_train[col].apply(str))
  X_test_embed = vectorizer.transform(X_test[col].apply(str))
  
  rating_train = np.concatenate((X_train_embed, rating_train), axis=1)
  rating_test = np.concatenate((X_test_embed, rating_test), axis=1)
  
  return rating_train, rating_test


for col in test_set.select_dtypes(include='object').columns:
  rating_train, rating_test = glove_embedded(train_set, col, test_set, rating_train, rating_test)

from xgboost import XGBRegressor
rating_model = XGBRegressor(objective='reg:squarederror', random_state=42)
rating_model.fit(rating_train, y_train)

#import pickle 
#filename = 'rating_model.sav'
#pickle.dump(rating_model, open(filename, 'wb'))

test_ratings = rating_model.predict(rating_test)

#add missing ratings to test dataset
test['rating'] = np.round(test_ratings, 2)

#combing train and test to form dataset
final_df = pd.concat([train, test], axis=0)
final_df.sort_index(inplace=True)

final_df.head()

"""#Calculating missing net experience"""

train = temp_df[~temp_df['net_experience'].isnull()]
test = temp_df[temp_df['net_experience'].isnull()]

train_indices = train.index.values
test_indices = test.index.values

train_set = train.drop(['net_experience', 'avg_yearly_sal'], axis=1)
y_train = train['net_experience']
test_set = test.drop(['net_experience', 'avg_yearly_sal'], axis=1)

#here we will apply glove embeddings on all text columns and concatenate them and them concatenate them along with numerical columns
"***Glove Embeddings***"

experience_train = train_set.select_dtypes(exclude='object').values
experience_test = test_set.select_dtypes(exclude='object').values

def glove_embedded(X_train, col, X_test, experience_train, experience_test):
  
  vectorizer = Word2VecVectorizer(model)

  X_train_embed = vectorizer.fit_transform(X_train[col].apply(str))
  X_test_embed = vectorizer.transform(X_test[col].apply(str))
  
  experience_train = np.concatenate((X_train_embed, experience_train), axis=1)
  experience_test = np.concatenate((X_test_embed, experience_test), axis=1)
  
  return experience_train, experience_test


for col in test_set.select_dtypes(include='object').columns:
  experience_train, experience_test = glove_embedded(train_set, col, test_set, experience_train, experience_test)

#will use xgboost to predict missing values

from xgboost import XGBRegressor

experience_model = XGBRegressor(objective= 'reg:squarederror', random_state=42)

experience_model.fit(experience_train, y_train)

#import pickle 
#filename = 'experience_model.sav'
#pickle.dump(experience_model, open(filename, 'wb'))

test_experience = experience_model.predict(experience_test)

test['net_experience'] = np.round(test_experience, 2)

temp = pd.DataFrame(columns=test.columns)

for idx in range(len(temp_df)):
  if idx in train_indices:
    temp = temp.append(train.loc[idx])
  elif idx in test_indices:
    temp = temp.append(test.loc[idx])

final_df['net_experience'] = temp['net_experience']

final_df.head()



"""#Transformation"""



from sklearn.feature_selection import VarianceThreshold

var_thres = VarianceThreshold(threshold=0)
var_thres.fit(final_df.select_dtypes(exclude='object'))


num_cols = final_df.select_dtypes(exclude='object').columns
final_df.drop(num_cols[~var_thres.get_support()], axis=1, inplace=True)

from scipy import stats
import pylab
#### Q-Q plot
def plot_data(df):
    plt.figure(figsize=(10,6))
    plt.subplot(1,2,1)
    df.hist(bins=20)
    plt.subplot(1,2,2)
    stats.probplot(df,dist='norm',plot=pylab)
    plt.show()

temp_df = final_df.copy()

for col in temp_df.select_dtypes(exclude = 'object').columns:
  print(col, '\n')
  plot_data(final_df[col])
  temp_df[col], _ = stats.boxcox(1+final_df[col])
  plot_data(temp_df[col])

for col in ['job_descr_len', 'net_experience', 'Cities']:
  final_df[col], _ = stats.boxcox(1+final_df[col])



"""#Create Dataset for Model building"""



final_df = final_df[(~final_df['avg_yearly_sal'].isnull()) & (final_df['avg_yearly_sal'] > 0)]

len(final_df)



final_df['avg_yearly_sal'] = final_df['avg_yearly_sal'].apply(lambda x: np.log(x))

#plot_data(final_df['avg_yearly_sal'])

from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(final_df.drop('avg_yearly_sal', axis=1), final_df['avg_yearly_sal'], test_size = 0.01,random_state=42)

X = final_df.drop('avg_yearly_sal', axis=1)
y = final_df['avg_yearly_sal']

X_train.reset_index(drop=True, inplace=True)
X_test.reset_index(drop=True, inplace=True)

y

X_train.head()

#here we will apply glove embeddings on all text columns and concatenate them and them concatenate them along with numerical columns
"***Glove Embeddings***"
from nlp_utils import Word2VecVectorizer
from gensim.models import KeyedVectors
# load GloVe model
filename = '/content/drive/MyDrive/Projects Datasets/word2vec_model.bin'
model = KeyedVectors.load_word2vec_format(filename, binary=True)


train_ans = X_train.select_dtypes(exclude='object').values
test_ans = X_test.select_dtypes(exclude='object').values

def glove_embedded(X_train, col, X_test, train_ans, test_ans):
  
  vectorizer = Word2VecVectorizer(model)
  X_train_embed = vectorizer.fit_transform(X_train[col].apply(str))
  X_test_embed = vectorizer.transform(X_test[col].apply(str))
  train_ans = np.concatenate((X_train_embed, train_ans), axis=1)
  test_ans = np.concatenate((X_test_embed, test_ans), axis=1)
  
  return train_ans, test_ans


for col in X_test.select_dtypes(include='object').columns:
  train_ans, test_ans = glove_embedded(X_train, col, X_test, train_ans, test_ans)

#here we will apply glove embeddings on all text columns and concatenate them and them concatenate them along with numerical columns
"***Glove Embeddings***"
from utils.nlp_utils import Word2VecVectorizer
from gensim.models import KeyedVectors
# load GloVe model
filename = '/content/drive/MyDrive/Projects Datasets/word2vec_model.bin'
model = KeyedVectors.load_word2vec_format(filename, binary=True)


train_ans = X.select_dtypes(exclude='object').values

def glove_embedded(X, col,train_ans):
  
  vectorizer = Word2VecVectorizer(model)
  X_embed = vectorizer.fit_transform(X[col].apply(str))
  train_ans = np.concatenate((X_embed, train_ans), axis=1)
  
  return train_ans


for col in X.select_dtypes(include='object').columns:
  train_ans = glove_embedded(X, col,train_ans)

train_ans.shape

"""
#Feature Selection
"""

"""#Feature Selection"""

from sklearn.feature_selection import VarianceThreshold

var_thres = VarianceThreshold(threshold=0)
var_thres.fit(train_ans)

np.unique(var_thres.get_support())

from sklearn.feature_selection import SelectFromModel
from xgboost import XGBRegressor

xgb_reg = XGBRegressor()

xgb_reg.fit(train_ans, y_train)

model = SelectFromModel(xgb_reg, prefit=True)

X_train_new = model.transform(train_ans)
X_test_new = model.transform(test_ans)

raw_df.iloc[0]



p = Preprocess_data()

temp_df = test_df

p.clean_data(temp_df)

p.final_operations(temp_df)







"""#Input Example"""

class Preprocess_data:

    def preprocess_text(self, text):
        text = text.lower()
        text = re.sub('[^a-zA-Z]', ' ', text)
        text = ' '.join([word for word in text.split() if not word in set(stopwords.words('english'))])
        text = ' '.join([lemmatizer.lemmatize(w) for w in text.split()])

        return text
    
    def clean_data(self, df):
        #Some company posted for the same profile many times after a gap of few days   
        # we can store this information in the column 'posting frequency'
        # calculate posting frequency on the basis of company
        freq = df['Company'].value_counts()

        df.drop_duplicates(inplace=True)
        df.reset_index(drop=True, inplace=True)

        df['posting_frequency'] = df['Company'].map(freq)

        # those not repeated will be null, therefore fill them as 1
        df['posting_frequency'].fillna(1, inplace=True)

        # We just deleted duplicates but we still see multiple entries for some companies
        # It looks like recently posted jobs with new tag are causing this, 
        # lets remove them
        df['Job_position'] = df['Job_position'].apply(lambda x: str(x).replace('\nnew','').lower())


        df['rating'] = pd.to_numeric(df['rating'])

        return df


    def work_location(self, df):

        work_type = []
        for j in range(len(df)):

            if re.findall('full.time?', df['requirements'][j].lower()):
                work_type.append(2)

            elif re.findall('remote?', df['requirements'][j].lower()):
                work_type.append(1)

            else:
                work_type.append(0)

        df['work_category'] = work_type

        return df



    def calc_experience(self, df):
        df['experience'].fillna('', inplace = True)

        experience_list = []
        for j in range(len(df)):

            if re.findall(r'\d+ year', df['experience'][j]):
                experience_list.append(re.search(r'\d+ year', df['experience'][j]).group()[0])

            else:
                experience_list.append('')

        df['net_experience'] = experience_list
        df['net_experience'] = pd.to_numeric(df['net_experience'])
                
        return df



    #Educational criteria mentioned by these companies can also be useful
    def education(self, df):

        education_list = []
        education_dict = {'bachelor':1, 'master':2, 'graduate':3}
        for j in range(len(df)):
            
            if re.findall(r'(graduate|bachelor|master)', df['experience'][j].lower()):
                education_list.append( education_dict[re.search(r'(graduate|bachelor|master)', df['experience'][j].lower()).group()] )
            
            elif re.findall(r'(graduate|bachelor|master)', df['requirements'][j].lower()):
                education_list.append( education_dict[re.search(r'(graduate|bachelor|master)', df['requirements'][j].lower()).group()] )
            
            else:
                education_list.append(0)


        df['education_level'] = education_list

        return df



    def seniority(self, df):

        seniority_list=[]
        for j in range(len(df)):

            if re.findall(r'senior', df['requirements'][j].lower()):
                seniority_list.append(2)
            
            elif re.findall(r'junior', df['requirements'][j].lower()):
                seniority_list.append(1)

            else:
                seniority_list.append(0)

        
        df['job_title'] = seniority_list

        return df


    def get_states(self, df):

        with open('utils/states.txt', 'r') as f:
            states = f.read()
            states_list = states.split(',')
        f.close()
        
        job_states = []

        for j in range(len(df)):
            counter = 0
            
            for i in states_list:
                if re.findall(i, df['Location'][j].lower()):
                    job_states.append(states_list.index(i))
                    counter = 1
                    break
            
            if counter == 0:
                job_states.append(states_list.index('State_missing'))

        df['State'] = job_states
        
        return df




    def city(self, df):

        with open('utils/cities.txt', 'r') as f:
            cities = f.read()
            cities_list = cities.split(',')
        f.close()

        job_cities = []

        for j in range(len(df)):
            counter = 0
            
            for i in cities_list:
                if re.findall(i, df['Location'][j].lower()):
                    job_cities.append(cities_list.index(i))
                    counter = 1
                    break
            
            if counter == 0:
                job_cities.append(cities_list.index('city_missing'))
                
        df['Cities'] = job_cities
        
        return df
    
    


    
    def analyze_skills(self, df):
        key_df = pd.read_csv('utils/keywords.csv')
        
        key_df.fillna('na', inplace=True)
        
        for col in key_df.columns:
            df[col] = np.zeros(len(df), dtype='int')
            
        for j in range(len(df)):
            for col in key_df.columns:
                for skill in key_df[col]:
                    try:
                        if skill != 'na':
                            if re.findall(skill, df['requirements'][j].lower()):
                                df.loc[j, col] += 1
                            
                            if re.findall(skill, df['Job_position'][j].lower()):
                                df.loc[j, col] += 1
                    except:
                        pass
        return df
    

    def final_operations(self, df):
    
        # remove columns with constant values
        df['Company'] = df['Company'].apply(lambda x: self.preprocess_text(str(x)))
        df['Job_position'] = df['Job_position'].apply(lambda x: self.preprocess_text(str(x)))
        df['requirements'] = df['requirements'].fillna('')
        df['requirements'] = df['requirements'].apply(lambda x: self.preprocess_text(str(x)))
        df['job_descr_len'] = df['requirements'].apply(lambda x: 0 if not x else len(x))
        df.drop(['experience', 'Location'], axis=1, inplace=True)

        return df
    
    def __call__(self, df):
        df = self.clean_data(df)
        df = self.work_location(df)
        df = self.calc_experience(df)
        df = self.education(df)
        df = self.seniority(df)
        df = self.get_states(df)
        df = self.city(df)
        df = self.analyze_skills(df)
        df = self.final_operations(df)
        return df

test_df = {'Job_position' : ['junior software developer'], 'Company':['gather network'],'Location': ['Urban Estate Gurgaon, Haryana'], 'requirements': ['actively looking fresher want start career blockchain technology extreme desire work closely industry'], 'rating':[''], 'experience': [''], 'posting_frequency': ['1']}

test_df = pd.DataFrame.from_dict(test_df)

test_df



test_df = Preprocess_data()(test_df)

test_df

from nlp_utils import Word2VecVectorizer
from gensim.models import KeyedVectors

model = KeyedVectors.load_word2vec_format('/content/drive/MyDrive/Projects Datasets/word2vec_model.bin', binary=True)
input_ = test_df.select_dtypes(exclude='object').values

def glove_embedded(X_train, col, train_ans):
  
  vectorizer = Word2VecVectorizer(model)
  X_train_embed = vectorizer.fit_transform(X_train[col])
  train_ans = np.concatenate((X_train_embed, train_ans), axis=1)
  
  return train_ans

for col in test_df.select_dtypes(include='object').columns:
  input_ = glove_embedded(test_df, col, input_)

import pickle
loaded_model = pickle.load(open("/content/xgb_model_gpu.sav", 'rb'))

loaded_model

pred = xgr.predict(input_)

input_.shape

np.exp(pred)

pred





"""#Model """

from sklearn.model_selection import GridSearchCV
from sklearn.metrics import mean_squared_error

# Lets tune our random forest for better performance
from sklearn.ensemble import RandomForestRegressor, VotingRegressor

rnd_reg = RandomForestRegressor(oob_score=True, random_state=42)
param_grid = {'n_estimators' : [500], 'min_samples_split':[3]}
grid = GridSearchCV(RandomForestRegressor(), param_grid=param_grid)
grid.fit(train_ans, y_train)
rnd_best = grid.best_estimator_
pred = rnd_best.predict(test_ans)

np.sqrt(mean_squared_error(np.exp(y_test), np.exp(pred)))

#SVR
from sklearn.svm import SVR
svr = SVR()
param_grid = {'C': [0.5, 1, 1.5]}
grid = GridSearchCV(svr, param_grid=param_grid)
grid.fit(train_ans, y_train)
svr_best = grid.best_estimator_
pred = svr_best.predict(test_ans)

np.sqrt(mean_squared_error(np.exp(y_test), np.exp(pred)))

#Lasso
from sklearn.linear_model import Lasso
lasso = Lasso(random_state=42)
param_grid = {'alpha': np.arange(1,101)/100, 'max_iter': [1000, 3000, 6000, 10000]} 
grid = GridSearchCV(lasso, param_grid=param_grid)
grid.fit(train_ans, y_train)
lasso_best = grid.best_estimator_
pred = lasso_best.predict(test_ans)

np.sqrt(mean_squared_error(np.exp(y_test), np.exp(pred)))

#DTree
from sklearn.tree import DecisionTreeRegressor
dtree = DecisionTreeRegressor(criterion='mae', random_state=42)
dtree.fit(train_ans, y_train)
pred = dtree.predict(test_ans)

np.sqrt(mean_squared_error(np.exp(y_test), np.exp(pred)))

#XGBoost
from xgboost import XGBRegressor

xgr = XGBRegressor(learning_rate = 0.1, max_depth = 5, random_state=42, objective = 'reg:squarederror', n_estimators = 800)
xgr.fit(train_ans, y_train)
#xgr_best = grid.best_estimator_
pred = xgr.predict(test_ans)

from sklearn.metrics import mean_squared_error
np.sqrt(mean_squared_error(np.exp(y_test), np.exp(pred)))

from xgboost import XGBRegressor

xgr = XGBRegressor(learning_rate = 0.1, max_depth = 5, random_state=42, objective = 'reg:squarederror', n_estimators = 800)
xgr.fit(train_ans, y)
#pred = xgr.predict(test_ans)

#np.sqrt(mean_squared_error(np.exp(y_test), np.exp(pred)))

#np.sqrt(mean_squared_error(np.exp(y_test), np.exp(pred)))

import pickle 
filename = 'xgb_model.sav'
pickle.dump(xgr, open(filename, 'wb'))


